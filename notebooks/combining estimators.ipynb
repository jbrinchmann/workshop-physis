{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods\n",
    "\n",
    "Ensemble methods are many and varied. They all make use of the fact that combining several simple learners can lead to a final estimator with both low bias and lower variance. This notebook shows some practical examples of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "This method uses bootstrap repetition of the sample to create a stable fit. This can be used for instance to create stable higher order polynomial fits or k-neighbours regression.\n",
    "\n",
    "To illustrate the use, let us first see how you can simply implement this yourself and then move to the `sklearn` implementation.\n",
    "\n",
    "The first step then is to create a simple random dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return np.sin(2*x)\n",
    "\n",
    "def draw_random_data(N=15):\n",
    "    \"\"\"Draw a few points from a sin(2*x).\n",
    "    \n",
    "    Return: x, y, dy. dy is a noise array that can be added to y to incorporate\n",
    "            some noise in the data.        \n",
    "    \"\"\"\n",
    "    np.random.seed(100)\n",
    "    x = np.random.uniform(-3, 3, N)\n",
    "    dy = np.random.normal(0, 0.4, size=N)\n",
    "    y = func(x)\n",
    "    return x, y, dy\n",
    "\n",
    "x, y, dy = draw_random_data()\n",
    "xtrue = np.linspace(-3, 3, 500)\n",
    "ytrue = func(xtrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.scatter(x, y+dy, marker='+', color='red')\n",
    "plt.plot(xtrue, ytrue, ls='dashed', color='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a bagging estimator from scratch\n",
    "\n",
    "We start here by creating a function for the regressor we will use. In this case I will use a k-nearest neighbour regressor, but you could substitute this function for something else. It is however generally most interesting to use a regressor that has some local structure. You will see below that I both to get the evaluation at the input points and at a x variable for plotting - this is of course for visual inspection of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.utils import resample\n",
    "\n",
    "def my_regression(x, y, k=3, weights='uniform', xplot=None):\n",
    "    \"\"\"The regression function to use for bagging.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise the regressor object\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, weights=weights)\n",
    "    \n",
    "    # Run the fit\n",
    "    knn.fit(x[:, np.newaxis], y)\n",
    "    \n",
    "    # Predict results\n",
    "    ypred = knn.predict(x[:, np.newaxis])\n",
    "    if xplot is not None:\n",
    "        yplot = knn.predict(xplot[:, np.newaxis])\n",
    "    else:\n",
    "        yplot = ypred\n",
    "    \n",
    "    return ypred, yplot, knn    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to run this fit. If we just fit our data once, then we just get the KNN regressor. In bagging we loop over many times, picking a random subsample with replacement each time. We will use the `sklearn.utils.resample` function to create a resampled array. To check how that works we can do the draw explicitly - note that of course `x2` and `y2` have the same number of elements as `x` and `y`, it is just that some are repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x2, y2 = resample(x, y, replace=True)\n",
    "plt.scatter(x2, y2, 200, color='blue', label='sampled')\n",
    "plt.scatter(x, y, color='red', marker='s', label='original')\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_bagging_regressor(x, y, k=3, weights='uniform', n_estimators=10):\n",
    "    \"\"\"\n",
    "    Carry out bagging regression using kNN regression\n",
    "    \"\"\"\n",
    "    \n",
    "    xplot = np.linspace(-3, 3, 200)\n",
    "    \n",
    "    # We carry out n_estimators fits\n",
    "    estimators = []\n",
    "    ypred = np.zeros((len(x), n_estimators))\n",
    "    yplot = np.zeros((len(xplot), n_estimators))\n",
    "    for i_est in range(n_estimators):\n",
    "        # Draw a resampled version with replacement\n",
    "        xr, yr = resample(x, y, replace=True)\n",
    "        \n",
    "        # Run the fit - note that I input the _RESAMPLED_ values\n",
    "        t_ypred, t_yplot, t_knn = my_regression(xr, yr, k=k, weights=weights, xplot=xplot)\n",
    "        ypred[:, i_est] = t_ypred\n",
    "        yplot[:, i_est] = t_yplot\n",
    "        estimators.append(t_knn)\n",
    "    \n",
    "    return ypred, estimators, xplot, yplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run this estimator. The `weight` option is also worth commenting on - the default is to simply take the average of the `k`-nearest neighbours. This is what you get if you set `weights='uniform'`, but it is also possible to add a weight to the estimates that is inversely proportional to the distance, so that more distant points are down-weighted. This can be achieved by setting `weights='distance'` - try both to get a feel for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = 10\n",
    "ypred, est, xp, yp = my_bagging_regressor(x, y, k=3, weights='distance', n_estimators=n_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the result for each individual run (because I wanted those for plotting). We do however usually only care about the averaged fits. So let us calculate the mean predicted y values both for the individual points and for the plot values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ym = np.mean(ypred, axis=1)\n",
    "ypm = np.mean(yp, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot this. The plot below shows 5 random fits as dotted gray lines and then the average line in red and the truth as a dashed black line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "inds = np.arange(n_est)\n",
    "np.random.shuffle(inds)\n",
    "for i_t in range(5):\n",
    "    i = inds[i_t]\n",
    "    plt.plot(xp, yp[:, i], color='gray', alpha=0.5, ls='dotted')\n",
    "plt.plot(xp, ypm, color='red')\n",
    "plt.plot(xtrue, ytrue, ls='dashed', color='black')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging using sklearn\n",
    "\n",
    "As you can see above, writing your own bagging function is not that hard, and will help you understand what is going on, but for your convenience, `sklearn` has a bagging package as well which we can now try.\n",
    "\n",
    "To use `sklearn.ensemble.BaggingRegressor` we need to define a regressor to use. I'll use k-nearest neighbours again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=3, weights='distance')\n",
    "bgr = BaggingRegressor(knn, n_estimators=n_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = bgr.fit(x[:, np.newaxis], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yplot_sk = r.predict(xp[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(xp, yplot_sk, color='red', label='sklearn')\n",
    "plt.plot(xp, ypm, color='green', label='Mine')\n",
    "plt.plot(xtrue, ytrue, ls='dashed', color='black')\n",
    "plt.legend(loc=3)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two do disagree a bit but the code should be pretty much identical so it should be mostly just random scatter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "The next ensemble method we discussed was boosting. This is also not very challenging to implement yourself but it is probably just as well to start directly with `sklearn` here. \n",
    "\n",
    "The best known of these techniques is the AdaBoost method. Here I will first use this on the same regression problem above, and then use it as a classifier on a real problem (classifying stars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first run in default mode where the regressor is a Decision tree regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = AdaBoostRegressor(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_reg = reg.fit(x[:, np.newaxis], y)\n",
    "yplot_boost = reg.predict(xp[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(xp, yplot_sk, color='red', label='Bagging')\n",
    "plt.plot(xp, yplot_boost, color='green', label='Boost')\n",
    "plt.plot(xtrue, ytrue, ls='dashed', color='black')\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run this with a k-nearest neighbours regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=3, weights='distance')\n",
    "knn_boost = AdaBoostRegressor(knn, n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_boost_knn = knn_boost.fit(x[:, np.newaxis], y)\n",
    "yplot_boost_knn = knn_boost.predict(xp[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(xp, yplot_sk, color='red', label='Bagging')\n",
    "plt.plot(xp, yplot_boost, color='green', label='Boost')\n",
    "plt.plot(xp, yplot_boost_knn, color='blue', label='Boost knn')\n",
    "plt.plot(xtrue, ytrue, ls='dashed', color='black')\n",
    "plt.legend(loc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(knn_boost, x[:, np.newaxis], y, scoring=\"neg_mean_squared_error\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Trying a more realistic example\n",
    "\n",
    "These kinds of methods are not particularly interesting for 1D fitting problems - these examples are used because it is easy to visualise the results. Let us now turn to a more realistic question. We will return to the colours of stars and trying to estimate the temperature of the stars from their colours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.datasets import fetch_sdss_sspp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have good internet connection you can do this:\n",
    "#d = fetch_sdss_sspp()\n",
    "#t = Table(d)\n",
    "\n",
    "# If you have the data downloaded, do this:\n",
    "t = Table().read(\"star_properties.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract some data. \n",
    "ug = data['upsf']-data['gpsf']\n",
    "gr = data['gpsf']-data['rpsf']\n",
    "ri = data['rpsf']-data['ipsf']\n",
    "iz = data['ipsf']-data['zpsf']\n",
    "T = data['Teff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with this we can now try bagging and boosting. For this we need the Bagging regressor - I take the very simple approach used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=3, weights='distance')\n",
    "knn_bag = BaggingRegressor(knn, n_estimators=20)\n",
    "knn_boost = AdaBoostRegressor(knn, n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the data matrix  (I called this `M` in the other notebook - sorry, noticed too late to change!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((ug, gr, ri, iz)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to see how well the methods work we need to divide into test, training and validation samples. Here I use a simple method for illustration to not clutter up the notebook with too much code, and not too much execution time. For the latter I will also subset the data to only use the first 10,000 points. You can comment out this and try it on the larger sample but it will then take a minute or so, depending on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I draw a random set of 10,000 objects. I set replace to False because I do not want to draw with\n",
    "# replacement. I also set random_state to a fixed number so my run is reproducible - in production code\n",
    "# this is not likely to be what you want.\n",
    "N_to_keep = 10000\n",
    "Xkeep, Tkeep = resample(X, T, n_samples=N_to_keep, replace=False, random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xkeep, Tkeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a few seconds\n",
    "T_est_bag = knn_bag.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_test-T_est_bag, marker='.')\n",
    "plt.xlabel('T')\n",
    "plt.ylabel(r'$T-T_{est}$')\n",
    "plt.ylim(-3000, 3000)\n",
    "plt.title('Bagging of KNN')\n",
    "sig = np.std(y_test-T_est_bag)\n",
    "plt.text(4500, 2500, r\"RMS={0:.2f}K\".format(sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So fairly reasonable but there are clearly outliers and some significant scatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_est_boost = knn_boost.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_test-T_est_boost, marker='.')\n",
    "plt.xlabel('T')\n",
    "plt.ylabel(r'$T-T_{est}$')\n",
    "plt.ylim(-3000, 3000)\n",
    "plt.title('Boosting of KNN')\n",
    "sig = np.std(y_test-T_est_boost)\n",
    "plt.text(4500, 2500, r\"RMS={0:.2f}K\".format(sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is comparable, but actually less well performing in the overall sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests\n",
    "\n",
    "The final ensemble estimator to consider is the random forest. A random forest is a combination of simple classification trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interface is the same as we saw above. The regression tree has quite a few parameters that can be used to tweak it. An interesting aspect here is the criterion - by default this is the mean squared error, but here I use mean absolute error as that should be less sensitive to outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=10, criterion='absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_est_rf = rf.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_test-T_est_rf, marker='.')\n",
    "plt.xlabel('T')\n",
    "plt.ylabel(r'$T-T_{est}$')\n",
    "plt.ylim(-3000, 3000)\n",
    "plt.title('Random forest')\n",
    "sig = np.std(y_test-T_est_rf)\n",
    "plt.text(4500, 2500, r\"RMS={0:.2f}K\".format(sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this performs the best of the three, but only very slightly better than the bagging approach But it is worth remembering here that this approach using a tree and the others use knn regression which might be more flexible here but which also could have more variance. To do a fair comparison you need to now wrap all this in a cross-validation loop. But that will take a bit of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Scaling\n",
    "\n",
    "But what about scaling, I hear you say. Up until here, I have no scaled the data. This is a valid concern, so let us try to use a standard scaler. For doing this I decided to go all the way back to get the original data and scale that. I could of course have calculated the scaling parameters myself, but this way also works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "Xs = scl.fit_transform(X)\n",
    "Xs_keep, Ts_keep = resample(Xs, T, n_samples=N_to_keep, replace=False, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs_keep, Ts_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts_est_rf = rf.fit(Xs_train, ys_train).predict(Xs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ys_test, ys_test-Ts_est_rf, marker='.')\n",
    "plt.xlabel('T')\n",
    "plt.ylabel(r'$T-T_{est}$')\n",
    "plt.ylim(-3000, 3000)\n",
    "sig = np.std(ys_test-Ts_est_rf)\n",
    "plt.text(4500, 2500, r\"RMS={0:.2f}K\".format(sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this gives pretty much exactly the same result as what we got when we did not scale. The reason for this is that the variables here span very much the same ranges even before scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: how do these methods compare with the linear regressors you tried out in the other notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
